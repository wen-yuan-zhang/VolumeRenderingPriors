<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VolumeRenderingPriors">
  <meta name="keywords" content="Novel view synthesis, Neural rendering, Radiance fields, Neural networks, Quadtree">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VolumeRenderingPriors</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://wen-yuan-zhang.github.io/Fast-Learning-NeRF/">
            Fast-Learning-NeRF
          </a>
          <a class="navbar-item" href="https://github.com/junshengzhou/VP2P-Match">
            VP2P-Match
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wen-yuan-zhang.github.io/">Wenyuan Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Kanle Shi<sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://h312h.github.io/">Zhizhong Han</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Software, Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Kuaishou Technology</span>
            <span class="author-block"><sup>3</sup>Wayne State University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wen-yuan-zhang/VolumeRenderingPriors"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px"></h2>

        <img src="./resources/teaser.png" class="center">

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Unsigned distance functions (UDFs) have been a vital representation for open surfaces. With different differentiable renderers, current methods are able to train neural networks to infer a UDF by minimizing the rendering errors on the UDF to the multi-view ground truth. However, these differentiable renderers are mainly handcrafted, which makes them either biased on ray-surface intersections, or sensitive to unsigned distance outliers, or not scalable to large scale scenes. To resolve these issues, we present a novel differentiable renderer to infer UDFs more accurately. Instead of using handcrafted equations, our differentiable renderer is a neural network which is pre-trained in a data-driven manner. It learns how to render unsigned distances into depth images, leading to a prior knowledge, dubbed volume rendering priors. To infer a UDF for an unseen scene from multiple RGB images, we generalize the learned volume rendering priors to map inferred unsigned distances in alpha blending for RGB image rendering. Our results show that the learned volume rendering priors are unbiased, robust, scalable, 3D aware, and more importantly, easy to learn. We evaluate our method on both widely used benchmarks and real scenes, and report superior performance over the state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method</h2>
        <img src="./resources/method.png" class="center">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            In this paper, we 
            (1) introduce volume rendering priors to infer UDFs from multi-view images. 
            Our prior can be learned in a data-driven manner, which provides a novel perspective to recover geometry with prior knowledge through volume rendering.
            (2) propose a novel deep neural network and learning scheme, 
            and report extensive analysis to learn an unbiased differentiable renderer for UDFs with robustness, scalability, and 3D awareness.
          </p>
          <p style="margin-top: 30px">
            Here is an overview of our method. 
            In the training phase, our volume rendering prior takes sliding windows of GT UDFs from training meshes as input, and outputs opaque densities for alpha blending. 
            The parameters are optimized by the error between rendered depth and ground truth depth maps. 
            During the testing phase, we freeze the volume rendering prior and use ground truth multi-view RGB images to optimize a randomly initialized UDF field.
          </p>
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Visualization Results</h2>


        <h3 class="title is-4">Comparison on Deepfashion3D Dataset</h3>
        <div class="content has-text-justified">
          <img src="./resources/deepfashion3d.png" class="center">
          <p>
            Visual comparisons on open surface reconstructions with error maps on DeepFashion3D dataset. Note that NeAT uses additional mask supervision. 
            The transition from blue to yellow indicates small to large reconstruction errors.
          </p>
        </div>

        <h3 class="title is-4">Comparison on DTU Dataset</h3>
        <div class="content has-text-justified">
          <img src="./resources/dtu.png" class="center">
        </div>

        <h3 class="title is-4">Comparison on Replica Dataset</h3>
        <div class="content has-text-justified">
          <img src="./resources/replica.png" class="center">
        </div>

        <h3 class="title is-4">Comparison on Real-Captured Datasets</h3>
        <div class="content has-text-justified">
          <img src="./resources/real-captured.png" class="center">
          <p>
            Comparison on Real-Captured Datasets. (a) is the visualization of our real-captured scenes. The first one is a paper-folding greeting card, and the second one is a plant.
            (b) is the visualization of the real scans used in NeUDF. The right-top and right-bottom part of each image are enlarged details and rendering views, respectively. 
          </p>
        </div>
          
        

        <h3 class="title is-4">Visualization Video</h3>
        <div class="content has-text-justified">
          <p>
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/video.mp4"
                    type="video/mp4">
          </video>
        </div>
        

      </div>
    </div>

  </div>
</section>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
     <pre><code>@article{zhang2024learning,
      title={Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors},
      author={Zhang, Wenyuan and Shi, Kanle and Liu, Yu-Shen and Han, Zhizhong},
      journal={European Conference on Computer Vision},
      year={2024},
      organization={Springer}
    }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
